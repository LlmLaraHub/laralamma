[
    {
        "content": "How can I have guardrails",
        "role": "user"
    },
    {
        "content": "You are a helpful assistant in the RAG system: \nThis is data from the search results when entering the users prompt which is \n\n\n### START PROMPT \nHow can I have guardrails \n### END PROMPT\n\nPlease use this with the following context and only this, summarize it for the user and return as markdown so I can render it and strip out and formatting like extra spaces, tabs, periods etc: \n\n### START Context\nEach utterance is encoded into a semantic vector space, creating a semantic \"map\". That map\nhelps us line up semantic meaning and canonical forms.\nUser queries are encoded into the same vector space, allowing us to identify if they have a\nsemantic similarity to an existing cano\nWhen a new user query comes in, we encode it into the same semantic vector space as our\nprevious utterance examples. Using that map of utterances and the canonical forms they belong\nto, we can identify when a user query is semantically similar to those utterance examples. If the\nquery is similar, we trigger the canonica s semantically similar to those utterance examples. If the\nquery is similar, we trigger the canonical form that those utterances belong to.\nWith that, we have a decision-making process in milliseconds rather than several seconds  as\nwith agents.\nUsing RAG with guardrails does have nuances that we should consider. First, we must define the\ncanonical forms. We could view the requirement of defining canonical forms in two ways: (1) we\n\"what is the llama 2 model?\"\n\"tell me about Meta's new LLM\"\n\"what are the differences between Falcon and Llama?\"\n1\n2\n3 \nThis website utilizes technologies\nsuch a these issues taken care of automatically. Google Docs can create PDFs but\nthey are not tagged. Converting a non-tagged PDF to a tagged one is a non-\ntrivial process that you should try to avoid. Creating PDFs with tags in the\nfirst place should be a preferable choice.\nGrackle by Product - GrackleDocs\nEdit description\nwww.grackledocs.com\nWritten by Joey Yang-Li\n2 Followers\nExpert in PDF Technology and Structured Documents\nFollow\nPdf Document Processing Retrieval Augmented Accessibility\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading.   discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing.  ferences and\nsubsequently deriving an answer. In this framework, the pivotal component\nis the accurate retrieval of pertinent information, which is critical for the\nefficacy of the RAG model.\nFigure 1. The workflow of Retrieval-Augmented Generation (RAG).\nHowever, the process of retrieval from PDF files is fraught with challenges.\nCommon issues include inaccuracies in text extraction and disarray in the\nrow-column relationships of tables inside PDF files. Thus, before RAG, we\nneed to convert large documents into retrievable content. The conversion\ninvolves several steps, as shown in Figure 2:\n ge documents into retrievable content. The conversion\ninvolves several steps, as shown in Figure 2:\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing.  resses the question of whether the quality of PDF\nparsing and chunking affects the outcomes of RAG. We will explore the\nchallenges, methodologies, and real-world case studies pertaining to this\nissue. It will include an examination of two types of methods in this field,\nnamely rule-based and deep learning-based methods, followed by empirical\nevaluations of their efficacy through practical examples.\n2 PDF Parsing & Chunking\n2.1 Challenges and Methods Overview\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowl t deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing.  hitectures. For language generation\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-\nart parametric-only seq2seq baseline.\nThis model was contributed by ola13.\nRetrieval-augmented generation (RAG) models combine the powers of pretrained dense retrieval\n(DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize\nto generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and\nfine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks  models, and\nfine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.  \nclass transformers.RagConfig\t<source>\n( vocab_size , is_encoder_decoder , prefix , bos_token_id ,\npad_token_id , eos_token_id , decoder_start_token_id , title_sep\n, doc_sep , n_docs , max_combined_length ,\nUsage tips\nRagConfig = None  = True  = None  = None  = None  = None  = None = ' \/ '  = ' \/\/ '  = 5  = 300   can be more deterministic about what should trigger a particular action, or (2) we lose out on the\ninnate decision-making ability of an LLM.\nRAG with guardrails allows us to insert a user query directly into an action  but it cannot\nrephrase the query, specify metadata filters, or decide how many results to return. On the other\nhand, using an agent makes those tasks easy. However, if we can infer these parameters\ndeterministically with code, we can include them without using an LLM.\nThat gives us an idea of how RAG with guardrails works and its pros and cons. Let's jump into the\nimplementat s us an idea of how RAG with guardrails works and its pros and cons. Let's jump into the\nimplementation itself.\nImplementing RAG with Guardrails\nTo implement RAG with guardrails, we will rely on the NVIDIA NeMo Guardrails library. The library\nprimarily focuses on AI safety by implementing \"guardrails\" as protective measures against\nunwanted interactions. However, we can also use these guardrails to trigger things like RAG.\nBuilding the Knowledge Base\nAs with every RAG use case, we must first create our knowledge base. For that, we will use a\nsmall dataset of Llama 2 related ArXiv papers stored ate our knowledge base. For that, we will use a\nsmall dataset of Llama 2 related ArXiv papers stored in Hugging Face  we download the\ndataset like so:\nIn[7]:\n!pip install qU \\\n    nemoguardrails \\\n    pineconeclient \\\n    datasets \\\n    openai\n1\n2\n3\n4\n5\n datasets load_dataset\ndata  load_dataset\n    split\n1\n2\n3\n4\n5\n6\n-\n==0.4.0\n-==2.2.2\n==2.14.3\n==0.27.8 \nfrom import\n=\t(\n\"jamescalam\/llama-2-arxiv-papers-chunked\",\n=\"train\" \nThis website utilizes technologies\nsuch as cookies to enable essential\nsite functionality, as well as for\nanalytics, personalization, and\ntargeted advertising purposes. To\n MAY 1, 2024\nUpcoming research at Stack Overflow\nAPRIL 30, 2024\nCollaborating smarter, not harder\nAPRIL 25, 2024\nWhat language should beginning programmers choose?\nAPRIL 15, 2024\nHow to succeed as a data engineer without the burnout\nLATEST PODCAST\nMAY 7, 2024\nReshaping the future of API platforms\nLogin with your stackoverflow.com account to take part in the discussion.\nLightDarkAuto\nStack Overflow for Teams\nPricing\tUse cases  coding model, preferably TFDPRQuestionEncoder, and\nthe generator can be any seq2seq model, preferably TFBartForConditionalGeneration.\nThe model can be initialized with a RagRetriever for end-to-end generation or used in\ncombination with the outputs of a retriever in multiple steps---see examples for more details.\nThe model is compatible any autoencoding model as the question_encoder and any seq2seq\nmodel with language model head as the generator. It has been tested with\nTFDPRQuestionEncoder as the question_encoder and TFBartForConditionalGeneration as the\ngenerator.\nThis model inherits from TF he question_encoder and TFBartForConditionalGeneration as the\ngenerator.\nThis model inherits from TFPreTrainedModel. Check the superclass documentation for the\ngeneric methods the library implements for all its model (such as downloading or saving,\nresizing the input embeddings, pruning heads etc.)\nThis model is also a Tensorflow keras.Model subclass. Use it as a regular TF 2.0 Keras Model and\nrefer to the TF 2.0 documentation for all matter related to general usage and behavior.\nThe model is in a developing state as it is now fully supports in eager-mode only, and may not\nbe exported in Saved  a developing state as it is now fully supports in eager-mode only, and may not\nbe exported in SavedModel format.\ncall\t<source>\n( input_ids\t, attention_mask\n, decoder_input_ids\n, decoder_attention_mask\t,\nencoder_outputs\t, past_key_values\nAlthough the recipe for forward pass needs to be defined within this function, one should call\nthe Module instance afterwards instead of this since the former takes care of running the pre\nand post processing steps while the latter silently ignores them.: TFModelInputType | None = None : np.ndarray |\ntf.Tensor | None = None : np.ndarray | tf.Tensor | None =\nNo ensor of shape (1,), optional, returned when labels is provided)  Language\nmodeling loss.\n\nlogits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size))\n Prediction scores of the language modeling head. The score is possibly marginalized over all\ndocuments for each vocabulary token.\n\ndoc_scores (torch.FloatTensor of shape (batch_size, config.n_docs))  Score between\neach retrieved document embeddings (see retrieved_doc_embeds) and\nquestion_encoder_last_hidden_state.\n\npast_key_values (List[torch.FloatTensor], optional, returned when use_cache=True is\npass _state.\n\npast_key_values (List[torch.FloatTensor], optional, returned when use_cache=True is\npassed or when config.use_cache=True)  List of torch.FloatTensor of length\nconfig.n_layers, with each tensor of shape (2, batch_size, num_heads,\nsequence_length, embed_size_per_head)).\n\nExpand 17 parameters  nk is retrieved for RAG, LLM is unable to perceive\nany meaningful information from it. Similar situation for Chunk 2.\nMoreover, the headers of the table only exist in Chunk 1, so the lower\npart of the table in Chunk 2 becomes meaningless.\n3. It cannot recognize the reading order of the content. The last line of\nChunk 5, Management Discussion and Analysis is actually located at\nthe top of the page, but is parsed as the last sentence in the result. This is\nbecause PyPDF parses the document by the storage order of the\ncharacters, instead of their reading order. This may cause chaotic res t by the storage order of the\ncharacters, instead of their reading order. This may cause chaotic results\nwhen faced with complex layouts.\nThe result on another case Case 2 features with a complex and cross-page\ntable is shown in Figure 15 in the Appendix.\n2.3 Deep learning-based method: ChatDOC PDF Parser\nNext, we turn our attention to the method of deep learning-based parsing,\nexemplified by our ChatDOC PDF Parser. The ChatDOC PDF Parser\n(https:\/\/pdfparser.io\/) has been trained on a corpus of over ten million\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\n ver ten million\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing. \n### END Context",
        "role": "assistant"
    },
    {
        "content": "When choosing a language for beginning programmers, Python stands out as an ideal option due to its intuitive syntax that emphasizes readability and simplicity, which is crucial for beginners. Its extensive range of libraries and frameworks makes it suitable for both web development with Django or Flask, data analysis using pandas and NumPy, artificial intelligence applications through scikit-learn, and more. The vast community and resources available also ensure that learning Python will equip you well for a broad spectrum of programming tasks in the future.\n\n### Answer\nPython",
        "role": "assistant"
    },
    {
        "content": "How can I have guardrails",
        "role": "user"
    },
    {
        "content": "You are a helpful assistant in the RAG system: \nThis is data from the search results when entering the users prompt which is \n\n\n### START PROMPT \nHow can I have guardrails \n### END PROMPT\n\nPlease use this with the following context and only this, summarize it for the user and return as markdown so I can render it and strip out and formatting like extra spaces, tabs, periods etc: \n\n### START Context\nEach utterance is encoded into a semantic vector space, creating a semantic \"map\". That map\nhelps us line up semantic meaning and canonical forms.\nUser queries are encoded into the same vector space, allowing us to identify if they have a\nsemantic similarity to an existing cano\nWhen a new user query comes in, we encode it into the same semantic vector space as our\nprevious utterance examples. Using that map of utterances and the canonical forms they belong\nto, we can identify when a user query is semantically similar to those utterance examples. If the\nquery is similar, we trigger the canonica s semantically similar to those utterance examples. If the\nquery is similar, we trigger the canonical form that those utterances belong to.\nWith that, we have a decision-making process in milliseconds rather than several seconds  as\nwith agents.\nUsing RAG with guardrails does have nuances that we should consider. First, we must define the\ncanonical forms. We could view the requirement of defining canonical forms in two ways: (1) we\n\"what is the llama 2 model?\"\n\"tell me about Meta's new LLM\"\n\"what are the differences between Falcon and Llama?\"\n1\n2\n3 \nThis website utilizes technologies\nsuch a these issues taken care of automatically. Google Docs can create PDFs but\nthey are not tagged. Converting a non-tagged PDF to a tagged one is a non-\ntrivial process that you should try to avoid. Creating PDFs with tags in the\nfirst place should be a preferable choice.\nGrackle by Product - GrackleDocs\nEdit description\nwww.grackledocs.com\nWritten by Joey Yang-Li\n2 Followers\nExpert in PDF Technology and Structured Documents\nFollow\nPdf Document Processing Retrieval Augmented Accessibility\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading.   discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing.  ferences and\nsubsequently deriving an answer. In this framework, the pivotal component\nis the accurate retrieval of pertinent information, which is critical for the\nefficacy of the RAG model.\nFigure 1. The workflow of Retrieval-Augmented Generation (RAG).\nHowever, the process of retrieval from PDF files is fraught with challenges.\nCommon issues include inaccuracies in text extraction and disarray in the\nrow-column relationships of tables inside PDF files. Thus, before RAG, we\nneed to convert large documents into retrievable content. The conversion\ninvolves several steps, as shown in Figure 2:\n ge documents into retrievable content. The conversion\ninvolves several steps, as shown in Figure 2:\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing.  resses the question of whether the quality of PDF\nparsing and chunking affects the outcomes of RAG. We will explore the\nchallenges, methodologies, and real-world case studies pertaining to this\nissue. It will include an examination of two types of methods in this field,\nnamely rule-based and deep learning-based methods, followed by empirical\nevaluations of their efficacy through practical examples.\n2 PDF Parsing & Chunking\n2.1 Challenges and Methods Overview\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowl t deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing.  hitectures. For language generation\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-\nart parametric-only seq2seq baseline.\nThis model was contributed by ola13.\nRetrieval-augmented generation (RAG) models combine the powers of pretrained dense retrieval\n(DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize\nto generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and\nfine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks  models, and\nfine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.  \nclass transformers.RagConfig\t<source>\n( vocab_size , is_encoder_decoder , prefix , bos_token_id ,\npad_token_id , eos_token_id , decoder_start_token_id , title_sep\n, doc_sep , n_docs , max_combined_length ,\nUsage tips\nRagConfig = None  = True  = None  = None  = None  = None  = None = ' \/ '  = ' \/\/ '  = 5  = 300   can be more deterministic about what should trigger a particular action, or (2) we lose out on the\ninnate decision-making ability of an LLM.\nRAG with guardrails allows us to insert a user query directly into an action  but it cannot\nrephrase the query, specify metadata filters, or decide how many results to return. On the other\nhand, using an agent makes those tasks easy. However, if we can infer these parameters\ndeterministically with code, we can include them without using an LLM.\nThat gives us an idea of how RAG with guardrails works and its pros and cons. Let's jump into the\nimplementat s us an idea of how RAG with guardrails works and its pros and cons. Let's jump into the\nimplementation itself.\nImplementing RAG with Guardrails\nTo implement RAG with guardrails, we will rely on the NVIDIA NeMo Guardrails library. The library\nprimarily focuses on AI safety by implementing \"guardrails\" as protective measures against\nunwanted interactions. However, we can also use these guardrails to trigger things like RAG.\nBuilding the Knowledge Base\nAs with every RAG use case, we must first create our knowledge base. For that, we will use a\nsmall dataset of Llama 2 related ArXiv papers stored ate our knowledge base. For that, we will use a\nsmall dataset of Llama 2 related ArXiv papers stored in Hugging Face  we download the\ndataset like so:\nIn[7]:\n!pip install qU \\\n    nemoguardrails \\\n    pineconeclient \\\n    datasets \\\n    openai\n1\n2\n3\n4\n5\n datasets load_dataset\ndata  load_dataset\n    split\n1\n2\n3\n4\n5\n6\n-\n==0.4.0\n-==2.2.2\n==2.14.3\n==0.27.8 \nfrom import\n=\t(\n\"jamescalam\/llama-2-arxiv-papers-chunked\",\n=\"train\" \nThis website utilizes technologies\nsuch as cookies to enable essential\nsite functionality, as well as for\nanalytics, personalization, and\ntargeted advertising purposes. To\n MAY 1, 2024\nUpcoming research at Stack Overflow\nAPRIL 30, 2024\nCollaborating smarter, not harder\nAPRIL 25, 2024\nWhat language should beginning programmers choose?\nAPRIL 15, 2024\nHow to succeed as a data engineer without the burnout\nLATEST PODCAST\nMAY 7, 2024\nReshaping the future of API platforms\nLogin with your stackoverflow.com account to take part in the discussion.\nLightDarkAuto\nStack Overflow for Teams\nPricing\tUse cases  coding model, preferably TFDPRQuestionEncoder, and\nthe generator can be any seq2seq model, preferably TFBartForConditionalGeneration.\nThe model can be initialized with a RagRetriever for end-to-end generation or used in\ncombination with the outputs of a retriever in multiple steps---see examples for more details.\nThe model is compatible any autoencoding model as the question_encoder and any seq2seq\nmodel with language model head as the generator. It has been tested with\nTFDPRQuestionEncoder as the question_encoder and TFBartForConditionalGeneration as the\ngenerator.\nThis model inherits from TF he question_encoder and TFBartForConditionalGeneration as the\ngenerator.\nThis model inherits from TFPreTrainedModel. Check the superclass documentation for the\ngeneric methods the library implements for all its model (such as downloading or saving,\nresizing the input embeddings, pruning heads etc.)\nThis model is also a Tensorflow keras.Model subclass. Use it as a regular TF 2.0 Keras Model and\nrefer to the TF 2.0 documentation for all matter related to general usage and behavior.\nThe model is in a developing state as it is now fully supports in eager-mode only, and may not\nbe exported in Saved  a developing state as it is now fully supports in eager-mode only, and may not\nbe exported in SavedModel format.\ncall\t<source>\n( input_ids\t, attention_mask\n, decoder_input_ids\n, decoder_attention_mask\t,\nencoder_outputs\t, past_key_values\nAlthough the recipe for forward pass needs to be defined within this function, one should call\nthe Module instance afterwards instead of this since the former takes care of running the pre\nand post processing steps while the latter silently ignores them.: TFModelInputType | None = None : np.ndarray |\ntf.Tensor | None = None : np.ndarray | tf.Tensor | None =\nNo ensor of shape (1,), optional, returned when labels is provided)  Language\nmodeling loss.\n\nlogits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size))\n Prediction scores of the language modeling head. The score is possibly marginalized over all\ndocuments for each vocabulary token.\n\ndoc_scores (torch.FloatTensor of shape (batch_size, config.n_docs))  Score between\neach retrieved document embeddings (see retrieved_doc_embeds) and\nquestion_encoder_last_hidden_state.\n\npast_key_values (List[torch.FloatTensor], optional, returned when use_cache=True is\npass _state.\n\npast_key_values (List[torch.FloatTensor], optional, returned when use_cache=True is\npassed or when config.use_cache=True)  List of torch.FloatTensor of length\nconfig.n_layers, with each tensor of shape (2, batch_size, num_heads,\nsequence_length, embed_size_per_head)).\n\nExpand 17 parameters  nk is retrieved for RAG, LLM is unable to perceive\nany meaningful information from it. Similar situation for Chunk 2.\nMoreover, the headers of the table only exist in Chunk 1, so the lower\npart of the table in Chunk 2 becomes meaningless.\n3. It cannot recognize the reading order of the content. The last line of\nChunk 5, Management Discussion and Analysis is actually located at\nthe top of the page, but is parsed as the last sentence in the result. This is\nbecause PyPDF parses the document by the storage order of the\ncharacters, instead of their reading order. This may cause chaotic res t by the storage order of the\ncharacters, instead of their reading order. This may cause chaotic results\nwhen faced with complex layouts.\nThe result on another case Case 2 features with a complex and cross-page\ntable is shown in Figure 15 in the Appendix.\n2.3 Deep learning-based method: ChatDOC PDF Parser\nNext, we turn our attention to the method of deep learning-based parsing,\nexemplified by our ChatDOC PDF Parser. The ChatDOC PDF Parser\n(https:\/\/pdfparser.io\/) has been trained on a corpus of over ten million\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\n ver ten million\nSign up to discover human stories that deepen your understanding of the\nworld.\nFree\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and\nhighlights.\nTell your story. Find your audience.\nMembership\nAccess the best member-only stories.\nSupport independent authors.\nListen to audio narrations.\nRead offline.\nJoin the Partner Program and earn for\nyour writing. \n### END Context",
        "role": "assistant"
    }
]